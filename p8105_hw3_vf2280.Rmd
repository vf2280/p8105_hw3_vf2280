---
title: "Homework 3 Solutions"
author: "Vasili Fokaidis"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: html_document
---

```{r}
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = 0.6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1

```{r}
data("instacart")
```

There are `r nrow(instacart)` rows and `r ncol(instacart)` columns in the instacart dataset. The data reflects the level of items in orders by user. There are user and order variables such as: user/order ID, order day, and order hour. And, there are item variables such as: product name, aisle, department, and some numeric codes.

How many aisles are there, and which aisles are the most items ordered from?

```{r}
instacart %>%
  count(aisle) %>%
  arrange(desc(n))
```

There are 134 different aisles and most items are ordered from the fresh vegetables, fresh fruits, packaged vegetables fruits, and yogurt aisles.


Make a plot!

```{r}
instacart %>%
  count(aisle) %>%
  filter(n > 10000) %>%
  mutate(
      aisle = factor(aisle),
      aisle = fct_reorder(aisle, n)
  ) %>%
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


Make a table!

```{r}
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>%
  count(product_name) %>%
  mutate(rank = min_rank(desc(n))) %>%
  filter(rank < 4) %>%
  arrange(aisle, rank) %>%
  knitr::kable()
```

Apple vs. Ice Cream Table

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```


## Problem 2

Load, tidy, and wrangle the accel data.
```{r}
accel_data = 
  read.csv("./data/accel_data.csv") %>%
  pivot_longer(
    activity.1:activity.1440,
    names_to = "minute",
    names_prefix = "activity.",
    values_to = "activity counts",
  ) %>%
  janitor::clean_names() %>%
  mutate(
    minute = as.numeric(minute),
    day = as.factor(day),
    weekend_vs_weekday = case_when(
      day == "Saturday" ~ "weekend",
      day == "Sunday" ~ "weekend",
      day == "Monday" ~ "weekday",
      day == "Tuesday" ~ "weekday",
      day == "Wednesday" ~ "weekday",
      day == "Thursday" ~ "weekday",
      day == "Friday" ~ "weekday",
    )
  )
```


Aggregate across minutes to create a total activity variable for each day and create a table.

```{r}
accel_data %>%
  filter(day %in% c("Friday", "Saturday", "Sunday", "Monday", "Tuesday", "Wednesday", "Thursday")) %>%
  group_by(week, day) %>%
  summarize(total_minutes = sum(minute), total_activity_counts = sum(activity_counts)) %>%
  pivot_wider(
    names_from = day,
    values_from = total_activity_counts
  )
```

Make a plot showing the 24-hour activity time courses for each day!

```{r}
accel_data
  
```

## Problem 3

```{r}
library(p8105.datasets)
data("ny_noaa")
```

Describe NY NOAA dataset.

```{r}
names(ny_noaa)
head(ny_noaa)
```

The NY NOAA dataset has `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. The dataset contains `r sum(is.na(ny_noaa))` total missing observations which could cause some issues in deducing important descriptive statistics. Important variables are the weather station ID, date of observation, precipitation (mm), snowfall, snow depth (mm), maximum temperature, and minimum temperature. Some interesting values are the max snowfall recorded which is `r max(pull(ny_noaa, snow))` (mm), the max precipitation recorded which is `r max(pull(ny_noaa, prcp))` (mm), and the latest year of recorded observations which are in `r max(pull(ny_noaa, year))`.

Do some data cleaning!

```{r}
ny_noaa =
ny_noaa %>%
  janitor::clean_names() %>%
   separate(
        date, 
        into = c('year', 'month', 'day'),
        convert = TRUE
      ) %>%
  mutate(
    tmax = as.numeric(tmax),
    tmin = as.numeric(tmin),
    tmax = tmax/10,
    tmin = tmin/10,
    prcp = prcp/10
  ) %>%
  drop_na()
```

The most commonly observed snowfall values (mm):

```{r}
sort(table(pull(ny_noaa, snow)),decreasing=TRUE)[2:3]
```

The most common values of snowfall are 25mm and 13mm. This could be because of the time of year the observations were successfully recorded. Perhaps, observations are also successfully measured in times of lighter snowfall.

Make a two-panel plot!

```{r not plotting?!}
avg_max_temp_jan_July_df =
  ny_noaa %>%
  group_by(month, year, id) %>%
  filter(month == 01, month == 07) %>%
  summarize(average_max_temp = mean(tmax)
  ) %>%
  ggplot(aes(x = month, y = average_max_temp, group = id, color = id)) +
  geom_point() +
  facet_grid(. ~ month)
```

Make a two-panel plot showing tmax vs. tmin for the full dataset.

```{r not plotting?!}
tmax_tmin_p =
  ny_noaa %>%
  summarize(average_max_temp = mean(tmax), average_min_temp = mean(tmin)
  ) %>%
  ggplot(aes(x = average_max_temp, y = average_min_temp)) +
  geom_boxplot() +
  labs(title = "Average Max Temp vs. Average Min Temp",
       x = "Average Max Temp",
       y = "Average Min Temp")

snow_fall_p =
  ny_noaa %>%
  group_by(year) %>%
  filter('snow' > 0, 'snow' < 100) %>%
  ggplot(aes(x = year, y = snow, color = year)) +
  labs(title = "Distribution of Snowfall (mm) by Year")
```







